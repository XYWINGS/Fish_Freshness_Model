{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T07:59:46.156750Z",
     "iopub.status.busy": "2025-03-19T07:59:46.156438Z",
     "iopub.status.idle": "2025-03-19T08:00:06.228761Z",
     "shell.execute_reply": "2025-03-19T08:00:06.227716Z",
     "shell.execute_reply.started": "2025-03-19T07:59:46.156715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T08:00:32.235386Z",
     "iopub.status.busy": "2025-03-19T08:00:32.234933Z",
     "iopub.status.idle": "2025-03-19T08:00:32.313312Z",
     "shell.execute_reply": "2025-03-19T08:00:32.312145Z",
     "shell.execute_reply.started": "2025-03-19T08:00:32.235348Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T08:00:36.460908Z",
     "iopub.status.busy": "2025-03-19T08:00:36.460472Z",
     "iopub.status.idle": "2025-03-19T08:00:36.466532Z",
     "shell.execute_reply": "2025-03-19T08:00:36.465325Z",
     "shell.execute_reply.started": "2025-03-19T08:00:36.460870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 2\n",
    "DATA_DIR = \"/kaggle/input/dataset/dataset\"\n",
    "MODEL_SAVE_PATH = \"multi_attribute_fish_model_novel.pth\"\n",
    "VIT_CONFIG_PATH = \"/kaggle/input/vitfiles/config.json\"\n",
    "VIT_MODEL_PATH = \"/kaggle/input/vitfiles/pytorch_model.bin\"\n",
    "RESNET18_WEIGHTS_PATH = \"/kaggle/input/pre-trained-resnet/resnet18-f37072fd.pth\"\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T08:00:45.617709Z",
     "iopub.status.busy": "2025-03-19T08:00:45.617393Z",
     "iopub.status.idle": "2025-03-19T08:00:45.626139Z",
     "shell.execute_reply": "2025-03-19T08:00:45.625236Z",
     "shell.execute_reply.started": "2025-03-19T08:00:45.617684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FishDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.classes = [\"fresh\", \"non_fresh\"]\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load image paths and labels\n",
    "        for label_idx, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            for attribute in [\"eyes\", \"gills\"]:  # Only eyes and gills\n",
    "                attribute_dir = os.path.join(class_dir, attribute)\n",
    "                for img_name in os.listdir(attribute_dir):\n",
    "                    self.image_paths.append((os.path.join(attribute_dir, img_name), attribute, label_idx))\n",
    "                    self.labels.append(label_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, attribute, label = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Apply domain-specific preprocessing\n",
    "        image = self.preprocess_image(image, attribute)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "        \n",
    "    def preprocess_image(self, image, attribute):\n",
    "        image = np.array(image)\n",
    "        \n",
    "        if attribute == \"eyes\":\n",
    "            # Reddish color detection for non-fresh eyes\n",
    "            hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            lower_red = np.array([0, 50, 50])  # Lower range for red\n",
    "            upper_red = np.array([10, 255, 255])  # Upper range for red\n",
    "            red_mask = cv2.inRange(hsv_image, lower_red, upper_red)\n",
    "            \n",
    "            # Glitter/reflectivity detection for fresh eyes\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            edges = cv2.Canny(gray_image, 100, 200)  # Detect edges\n",
    "            \n",
    "            # Combine masks\n",
    "            combined_mask = cv2.bitwise_or(red_mask, edges)\n",
    "            image = cv2.bitwise_and(image, image, mask=combined_mask)\n",
    "        \n",
    "        elif attribute == \"gills\":\n",
    "            # Enhance color contrast for gills\n",
    "            lab_image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "            l_channel, a_channel, b_channel = cv2.split(lab_image)\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "            l_channel = clahe.apply(l_channel)\n",
    "            lab_image = cv2.merge((l_channel, a_channel, b_channel))\n",
    "            image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2RGB)\n",
    "        \n",
    "        return Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Transformations with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T08:00:52.079515Z",
     "iopub.status.busy": "2025-03-19T08:00:52.079226Z",
     "iopub.status.idle": "2025-03-19T08:00:52.084772Z",
     "shell.execute_reply": "2025-03-19T08:00:52.083817Z",
     "shell.execute_reply.started": "2025-03-19T08:00:52.079493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing Transformations with Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
    "    transforms.RandomRotation(10),     # Randomly rotate images by Â±10 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Randomly adjust brightness, contrast, and saturation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T08:00:56.620324Z",
     "iopub.status.busy": "2025-03-19T08:00:56.619944Z",
     "iopub.status.idle": "2025-03-19T08:00:57.850759Z",
     "shell.execute_reply": "2025-03-19T08:00:57.849843Z",
     "shell.execute_reply.started": "2025-03-19T08:00:56.620294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = FishDataset(data_dir=os.path.join(DATA_DIR, \"train\"), transform=transform)\n",
    "valid_dataset = FishDataset(data_dir=os.path.join(DATA_DIR, \"valid\"), transform=transform)\n",
    "test_dataset = FishDataset(data_dir=os.path.join(DATA_DIR, \"test\"), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T08:01:00.941872Z",
     "iopub.status.busy": "2025-03-19T08:01:00.941508Z",
     "iopub.status.idle": "2025-03-19T08:01:00.950873Z",
     "shell.execute_reply": "2025-03-19T08:01:00.949852Z",
     "shell.execute_reply.started": "2025-03-19T08:01:00.941843Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiAttributeFishModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(MultiAttributeFishModel, self).__init__()\n",
    "        \n",
    "        # Pre-trained CNNs for feature extraction\n",
    "        self.eye_cnn = models.resnet18(pretrained=False)\n",
    "        self.eye_cnn.load_state_dict(torch.load(RESNET18_WEIGHTS_PATH))\n",
    "        \n",
    "        self.gill_cnn = models.resnet18(pretrained=False)\n",
    "        self.gill_cnn.load_state_dict(torch.load(RESNET18_WEIGHTS_PATH))\n",
    "        \n",
    "        # Modify CNNs for single-channel output\n",
    "        self.eye_cnn.fc = nn.Linear(self.eye_cnn.fc.in_features, 128)\n",
    "        self.gill_cnn.fc = nn.Linear(self.gill_cnn.fc.in_features, 128)\n",
    "        \n",
    "        # Vision Transformer for global context\n",
    "        vit_config = ViTConfig.from_pretrained(VIT_CONFIG_PATH)\n",
    "        self.vit = ViTModel(vit_config)\n",
    "\n",
    "        # Load the state dictionary and remove the 'vit.' prefix\n",
    "        state_dict = torch.load(VIT_MODEL_PATH)\n",
    "        state_dict = {k.replace(\"vit.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "        # Remove classifier-related keys\n",
    "        state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"classifier.\")}\n",
    "\n",
    "        # Load the modified state dictionary into the ViT model\n",
    "        self.vit.load_state_dict(state_dict, strict=False)  # Set strict=False to ignore missing keys\n",
    "\n",
    "        self.vit_fc = nn.Linear(self.vit.config.hidden_size, 128)\n",
    "        \n",
    "        # Attention Mechanism for Eyes and Gills\n",
    "        self.eye_attention = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.gill_attention = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Weighted Fusion Layer\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(128 * 3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, eye_img, gill_img):\n",
    "        # Extract features using CNNs\n",
    "        eye_features = self.eye_cnn(eye_img)  # [batch_size, 128]\n",
    "        gill_features = self.gill_cnn(gill_img)  # [batch_size, 128]\n",
    "        \n",
    "        # Apply Attention Mechanisms\n",
    "        eye_weights = self.eye_attention(eye_features)  # [batch_size, 1]\n",
    "        gill_weights = self.gill_attention(gill_features)  # [batch_size, 1]\n",
    "        \n",
    "        eye_features = eye_features * eye_weights  # Weighted features\n",
    "        gill_features = gill_features * gill_weights  # Weighted features\n",
    "        \n",
    "        # Extract global context using ViT\n",
    "        vit_outputs = self.vit(eye_img)  # Use eye_img as input to ViT\n",
    "        vit_features = self.vit_fc(vit_outputs.last_hidden_state.mean(dim=1))  # [batch_size, 128]\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([eye_features, gill_features, vit_features], dim=1)  # [batch_size, 128 * 3]\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fusion_fc(combined_features)  # [batch_size, num_classes]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model, Loss, Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T08:01:07.287130Z",
     "iopub.status.busy": "2025-03-19T08:01:07.286720Z",
     "iopub.status.idle": "2025-03-19T08:01:13.560245Z",
     "shell.execute_reply": "2025-03-19T08:01:13.559606Z",
     "shell.execute_reply.started": "2025-03-19T08:01:07.287095Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "<ipython-input-7-6b0158b803a4>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.eye_cnn.load_state_dict(torch.load(RESNET18_WEIGHTS_PATH))\n",
      "<ipython-input-7-6b0158b803a4>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.gill_cnn.load_state_dict(torch.load(RESNET18_WEIGHTS_PATH))\n",
      "<ipython-input-7-6b0158b803a4>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(VIT_MODEL_PATH)\n"
     ]
    }
   ],
   "source": [
    "# Model. Loss, Optimizer\n",
    "model = MultiAttributeFishModel(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)  # Add weight decay\n",
    "\n",
    "# Initialize the scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T08:01:15.715237Z",
     "iopub.status.busy": "2025-03-19T08:01:15.714958Z",
     "iopub.status.idle": "2025-03-19T09:02:18.776031Z",
     "shell.execute_reply": "2025-03-19T09:02:18.774917Z",
     "shell.execute_reply.started": "2025-03-19T08:01:15.715215Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Training Loss: 0.6362\n",
      "Validation Loss: 0.5665, Validation Accuracy: 73.91%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [2/25], Training Loss: 0.5739\n",
      "Validation Loss: 0.5092, Validation Accuracy: 73.91%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [3/25], Training Loss: 0.5616\n",
      "Validation Loss: 0.4798, Validation Accuracy: 78.53%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [4/25], Training Loss: 0.5574\n",
      "Validation Loss: 0.7232, Validation Accuracy: 63.56%\n",
      "Epoch [5/25], Training Loss: 0.5091\n",
      "Validation Loss: 0.5234, Validation Accuracy: 79.30%\n",
      "Epoch [6/25], Training Loss: 0.5275\n",
      "Validation Loss: 0.4348, Validation Accuracy: 80.58%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [7/25], Training Loss: 0.4949\n",
      "Validation Loss: 0.4789, Validation Accuracy: 79.98%\n",
      "Epoch [8/25], Training Loss: 0.5143\n",
      "Validation Loss: 0.4224, Validation Accuracy: 81.86%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [9/25], Training Loss: 0.4768\n",
      "Validation Loss: 0.3401, Validation Accuracy: 86.14%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [10/25], Training Loss: 0.4474\n",
      "Validation Loss: 0.3511, Validation Accuracy: 82.38%\n",
      "Epoch [11/25], Training Loss: 0.4627\n",
      "Validation Loss: 0.3711, Validation Accuracy: 84.35%\n",
      "Epoch [12/25], Training Loss: 0.4451\n",
      "Validation Loss: 0.3311, Validation Accuracy: 85.71%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [13/25], Training Loss: 0.4363\n",
      "Validation Loss: 0.4355, Validation Accuracy: 81.86%\n",
      "Epoch [14/25], Training Loss: 0.4104\n",
      "Validation Loss: 0.4748, Validation Accuracy: 78.19%\n",
      "Epoch [15/25], Training Loss: 0.4054\n",
      "Validation Loss: 0.3578, Validation Accuracy: 84.17%\n",
      "Epoch [16/25], Training Loss: 0.3981\n",
      "Validation Loss: 0.3613, Validation Accuracy: 85.80%\n",
      "Epoch [17/25], Training Loss: 0.3246\n",
      "Validation Loss: 0.2790, Validation Accuracy: 88.45%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [18/25], Training Loss: 0.3096\n",
      "Validation Loss: 0.2702, Validation Accuracy: 89.14%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [19/25], Training Loss: 0.2983\n",
      "Validation Loss: 0.2627, Validation Accuracy: 89.48%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [20/25], Training Loss: 0.2900\n",
      "Validation Loss: 0.2544, Validation Accuracy: 89.82%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [21/25], Training Loss: 0.2630\n",
      "Validation Loss: 0.2674, Validation Accuracy: 89.56%\n",
      "Epoch [22/25], Training Loss: 0.2748\n",
      "Validation Loss: 0.2528, Validation Accuracy: 89.91%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [23/25], Training Loss: 0.2595\n",
      "Validation Loss: 0.2849, Validation Accuracy: 89.48%\n",
      "Epoch [24/25], Training Loss: 0.2589\n",
      "Validation Loss: 0.2499, Validation Accuracy: 90.08%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [25/25], Training Loss: 0.2405\n",
      "Validation Loss: 0.2736, Validation Accuracy: 90.25%\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    " # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {val_loss/len(valid_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "  # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T09:04:36.219122Z",
     "iopub.status.busy": "2025-03-19T09:04:36.218627Z",
     "iopub.status.idle": "2025-03-19T09:04:42.290090Z",
     "shell.execute_reply": "2025-03-19T09:04:42.289032Z",
     "shell.execute_reply.started": "2025-03-19T09:04:36.219077Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2680, Test Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(images, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T12:03:41.853144Z",
     "iopub.status.busy": "2025-03-15T12:03:41.852751Z",
     "iopub.status.idle": "2025-03-15T12:52:40.584462Z",
     "shell.execute_reply": "2025-03-15T12:52:40.583538Z",
     "shell.execute_reply.started": "2025-03-15T12:03:41.853109Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-26d0e9ecfa5a>:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.eye_cnn.load_state_dict(torch.load(RESNET18_WEIGHTS_PATH))\n",
      "<ipython-input-5-26d0e9ecfa5a>:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.gill_cnn.load_state_dict(torch.load(RESNET18_WEIGHTS_PATH))\n",
      "<ipython-input-5-26d0e9ecfa5a>:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(VIT_MODEL_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.6340\n",
      "Validation Loss: 0.5045, Validation Accuracy: 76.05%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [2/20], Training Loss: 0.5636\n",
      "Validation Loss: 1.0206, Validation Accuracy: 56.63%\n",
      "Epoch [3/20], Training Loss: 0.5668\n",
      "Validation Loss: 1.1902, Validation Accuracy: 71.34%\n",
      "Epoch [4/20], Training Loss: 0.5358\n",
      "Validation Loss: 0.4391, Validation Accuracy: 79.64%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [5/20], Training Loss: 0.4993\n",
      "Validation Loss: 0.4342, Validation Accuracy: 79.64%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [6/20], Training Loss: 0.4996\n",
      "Validation Loss: 0.4102, Validation Accuracy: 79.98%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [7/20], Training Loss: 0.4732\n",
      "Validation Loss: 0.4225, Validation Accuracy: 80.07%\n",
      "Epoch [8/20], Training Loss: 0.4864\n",
      "Validation Loss: 0.4235, Validation Accuracy: 80.58%\n",
      "Epoch [9/20], Training Loss: 0.4787\n",
      "Validation Loss: 0.6019, Validation Accuracy: 67.58%\n",
      "Epoch [10/20], Training Loss: 0.4510\n",
      "Validation Loss: 0.3894, Validation Accuracy: 81.44%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [11/20], Training Loss: 0.4224\n",
      "Validation Loss: 0.3368, Validation Accuracy: 85.89%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [12/20], Training Loss: 0.4103\n",
      "Validation Loss: 0.4464, Validation Accuracy: 79.81%\n",
      "Epoch [13/20], Training Loss: 0.3934\n",
      "Validation Loss: 0.3820, Validation Accuracy: 83.92%\n",
      "Epoch [14/20], Training Loss: 0.4096\n",
      "Validation Loss: 0.3134, Validation Accuracy: 87.34%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [15/20], Training Loss: 0.3822\n",
      "Validation Loss: 0.3729, Validation Accuracy: 85.71%\n",
      "Epoch [16/20], Training Loss: 0.3817\n",
      "Validation Loss: 0.3611, Validation Accuracy: 84.43%\n",
      "Epoch [17/20], Training Loss: 0.4387\n",
      "Validation Loss: 0.4988, Validation Accuracy: 84.17%\n",
      "Epoch [18/20], Training Loss: 0.3789\n",
      "Validation Loss: 0.3516, Validation Accuracy: 83.40%\n",
      "Epoch [19/20], Training Loss: 0.3213\n",
      "Validation Loss: 0.2663, Validation Accuracy: 89.31%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Epoch [20/20], Training Loss: 0.2863\n",
      "Validation Loss: 0.2489, Validation Accuracy: 90.59%\n",
      "Model saved to multi_attribute_fish_model_novel.pth\n",
      "Test Loss: 0.3195, Test Accuracy: 86.90%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 2  # Fresh and Non-Fresh\n",
    "DATA_DIR = \"/kaggle/input/dataset/dataset\"\n",
    "MODEL_SAVE_PATH = \"multi_attribute_fish_model_novel.pth\"\n",
    "# Path to the uploaded ViT files\n",
    "VIT_CONFIG_PATH = \"/kaggle/input/vitfiles/config.json\"\n",
    "VIT_MODEL_PATH = \"/kaggle/input/vitfiles/pytorch_model.bin\"\n",
    "# Path to the uploaded weights file\n",
    "RESNET18_WEIGHTS_PATH = \"/kaggle/input/pre-trained-resnet/resnet18-f37072fd.pth\"\n",
    "\n",
    "# Dataset Class\n",
    "class FishDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.classes = [\"fresh\", \"non_fresh\"]\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load image paths and labels\n",
    "        for label_idx, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            for attribute in [\"eyes\", \"gills\"]:  # Only eyes and gills\n",
    "                attribute_dir = os.path.join(class_dir, attribute)\n",
    "                for img_name in os.listdir(attribute_dir):\n",
    "                    self.image_paths.append((os.path.join(attribute_dir, img_name), attribute, label_idx))\n",
    "                    self.labels.append(label_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, attribute, label = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Apply domain-specific preprocessing\n",
    "        image = self.preprocess_image(image, attribute)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "    def preprocess_image(self, image, attribute):\n",
    "        image = np.array(image)\n",
    "        \n",
    "        if attribute == \"eyes\":\n",
    "            # Reddish color detection for non-fresh eyes\n",
    "            hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            lower_red = np.array([0, 50, 50])  # Lower range for red\n",
    "            upper_red = np.array([10, 255, 255])  # Upper range for red\n",
    "            red_mask = cv2.inRange(hsv_image, lower_red, upper_red)\n",
    "            \n",
    "            # Glitter/reflectivity detection for fresh eyes\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            edges = cv2.Canny(gray_image, 100, 200)  # Detect edges\n",
    "            \n",
    "            # Combine masks\n",
    "            combined_mask = cv2.bitwise_or(red_mask, edges)\n",
    "            image = cv2.bitwise_and(image, image, mask=combined_mask)\n",
    "        \n",
    "        elif attribute == \"gills\":\n",
    "            # Enhance color contrast for gills\n",
    "            lab_image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "            l_channel, a_channel, b_channel = cv2.split(lab_image)\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "            l_channel = clahe.apply(l_channel)\n",
    "            lab_image = cv2.merge((l_channel, a_channel, b_channel))\n",
    "            image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2RGB)\n",
    "        \n",
    "        return Image.fromarray(image)\n",
    "\n",
    "# Preprocessing Transformations with Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
    "    transforms.RandomRotation(10),     # Randomly rotate images by Â±10 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Randomly adjust brightness, contrast, and saturation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# Load Datasets\n",
    "train_dataset = FishDataset(data_dir=os.path.join(DATA_DIR, \"train\"), transform=transform)\n",
    "valid_dataset = FishDataset(data_dir=os.path.join(DATA_DIR, \"valid\"), transform=transform)\n",
    "test_dataset = FishDataset(data_dir=os.path.join(DATA_DIR, \"test\"), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Model Definition\n",
    "class MultiAttributeFishModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(MultiAttributeFishModel, self).__init__()\n",
    "        \n",
    "        # Pre-trained CNNs for feature extraction\n",
    "        self.eye_cnn = models.resnet18(pretrained=False)\n",
    "        self.eye_cnn.load_state_dict(torch.load(RESNET18_WEIGHTS_PATH))\n",
    "        \n",
    "        self.gill_cnn = models.resnet18(pretrained=False)\n",
    "        self.gill_cnn.load_state_dict(torch.load(RESNET18_WEIGHTS_PATH))\n",
    "        \n",
    "        # Modify CNNs for single-channel output\n",
    "        self.eye_cnn.fc = nn.Linear(self.eye_cnn.fc.in_features, 128)\n",
    "        self.gill_cnn.fc = nn.Linear(self.gill_cnn.fc.in_features, 128)\n",
    "        \n",
    "        # Vision Transformer for global context\n",
    "        vit_config = ViTConfig.from_pretrained(VIT_CONFIG_PATH)\n",
    "        self.vit = ViTModel(vit_config)\n",
    "\n",
    "        # Load the state dictionary and remove the 'vit.' prefix\n",
    "        state_dict = torch.load(VIT_MODEL_PATH)\n",
    "        state_dict = {k.replace(\"vit.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "        # Remove classifier-related keys\n",
    "        state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"classifier.\")}\n",
    "\n",
    "        # Load the modified state dictionary into the ViT model\n",
    "        self.vit.load_state_dict(state_dict, strict=False)  # Set strict=False to ignore missing keys\n",
    "\n",
    "        self.vit_fc = nn.Linear(self.vit.config.hidden_size, 128)\n",
    "        \n",
    "        # Attention Mechanism for Eyes and Gills\n",
    "        self.eye_attention = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Add dropout\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.gill_attention = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Add dropout\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Weighted Fusion Layer\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(128 * 3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Add dropout\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, eye_img, gill_img):\n",
    "        # Extract features using CNNs\n",
    "        eye_features = self.eye_cnn(eye_img)  # [batch_size, 128]\n",
    "        gill_features = self.gill_cnn(gill_img)  # [batch_size, 128]\n",
    "        \n",
    "        # Apply Attention Mechanisms\n",
    "        eye_weights = self.eye_attention(eye_features)  # [batch_size, 1]\n",
    "        gill_weights = self.gill_attention(gill_features)  # [batch_size, 1]\n",
    "        \n",
    "        eye_features = eye_features * eye_weights  # Weighted features\n",
    "        gill_features = gill_features * gill_weights  # Weighted features\n",
    "        \n",
    "        # Extract global context using ViT\n",
    "        vit_outputs = self.vit(eye_img)  # Use eye_img as input to ViT\n",
    "        vit_features = self.vit_fc(vit_outputs.last_hidden_state.mean(dim=1))  # [batch_size, 128]\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([eye_features, gill_features, vit_features], dim=1)  # [batch_size, 128 * 3]\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fusion_fc(combined_features)  # [batch_size, num_classes]\n",
    "        return output\n",
    "\n",
    "# Initialize Model, Loss, and Optimizer\n",
    "model = MultiAttributeFishModel(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)  # Add weight decay\n",
    "\n",
    "# Initialize the scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "# Training and Validation Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {val_loss/len(valid_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "# Test the Model\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(images, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6875933,
     "sourceId": 11039061,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6875999,
     "sourceId": 11039145,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6876028,
     "sourceId": 11039192,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
